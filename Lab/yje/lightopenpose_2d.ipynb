{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "lightopenpose_2d.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Fbzs55_T0Wn"
      },
      "source": [
        "# Openpose light weight Test\n",
        " - https://github.com/Daniil-Osokin/lightweight-human-pose-estimation.pytorch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-t1V6Hb48b_Z"
      },
      "source": [
        "# lightweight-human-pose-estimation.pytorch Setting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "goCOGasgrY6K",
        "outputId": "85d153b0-e534-437b-bb59-18c87debf848",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "%cd /content/\n",
        "\n",
        "import os\n",
        "from os.path import exists, join, basename, splitext\n",
        "\n",
        "git_repo_url = 'https://github.com/Daniil-Osokin/lightweight-human-pose-estimation.pytorch.git'\n",
        "git_path = '/content/lightweight-human-pose-estimation.pytorch'\n",
        "pre_model_path = os.path.join(git_path,'pre_model')\n",
        "\n",
        "project_name = splitext(basename(git_repo_url))[0]\n",
        "if not exists(project_name):\n",
        "  !git clone -q --depth 1 $git_repo_url\n",
        "  !wget \"http://images.cocodataset.org/annotations/annotations_trainval2017.zip\" -P $git_path\n",
        "  !wget \"https://download.01.org/opencv/openvino_training_extensions/models/human_pose_estimation/checkpoint_iter_370000.pth\" -P $pre_model_path\n",
        "  !mkdir coco && unzip \"/content/lightweight-human-pose-estimation.pytorch/annotations_trainval2017.zip\" -d \"/content/coco/\"\n",
        "\n",
        "%cd $git_path"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "/content/lightweight-human-pose-estimation.pytorch\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zjEmOxpY0jco",
        "outputId": "2d163fcb-7842-4c24-f5b1-835937366cca",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!pip install -r requirements.txt"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 1)) (2.5.1+cu121)\n",
            "Requirement already satisfied: torchvision>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 2)) (0.20.1+cu121)\n",
            "Requirement already satisfied: pycocotools==2.0.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 3)) (2.0.0)\n",
            "Requirement already satisfied: opencv-python>=3.4.0.14 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 4)) (4.10.0.84)\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 5)) (1.26.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.1->-r requirements.txt (line 1)) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.1->-r requirements.txt (line 1)) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.1->-r requirements.txt (line 1)) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.1->-r requirements.txt (line 1)) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.1->-r requirements.txt (line 1)) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.1->-r requirements.txt (line 1)) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=0.4.1->-r requirements.txt (line 1)) (1.3.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision>=0.2.1->-r requirements.txt (line 2)) (11.0.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=0.4.1->-r requirements.txt (line 1)) (3.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "klz9GqW385tC"
      },
      "source": [
        "# Demo - mp4\n",
        " - colab ( demo_test.py )"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aZa73tDw_q0J"
      },
      "source": [
        "!rm -f demo_test.py"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OuFdh2drVijb"
      },
      "source": [
        "## create demo_test.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8EqediNK78T9",
        "outputId": "7f8ddea2-55db-4175-9305-048c3362dc92",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "%%writefile demo_test.py\n",
        "import argparse\n",
        "\n",
        "import cv2\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "from models.with_mobilenet import PoseEstimationWithMobileNet\n",
        "from modules.keypoints import extract_keypoints, group_keypoints\n",
        "from modules.load_state import load_state\n",
        "from modules.pose import Pose, track_poses\n",
        "from val import normalize, pad_width\n",
        "\n",
        "\n",
        "class ImageReader(object):\n",
        "    def __init__(self, file_names):\n",
        "        self.file_names = file_names\n",
        "        self.max_idx = len(file_names)\n",
        "\n",
        "    def __iter__(self):\n",
        "        self.idx = 0\n",
        "        return self\n",
        "\n",
        "    def __next__(self):\n",
        "        if self.idx == self.max_idx:\n",
        "            raise StopIteration\n",
        "        img = cv2.imread(self.file_names[self.idx], cv2.IMREAD_COLOR)\n",
        "        if img.size == 0:\n",
        "            raise IOError('Image {} cannot be read'.format(self.file_names[self.idx]))\n",
        "        self.idx = self.idx + 1\n",
        "        return img\n",
        "\n",
        "\n",
        "class VideoReader(object):\n",
        "    def __init__(self, file_name):\n",
        "        self.file_name = file_name\n",
        "        try:  # OpenCV needs int to read from webcam\n",
        "            self.file_name = int(file_name)\n",
        "        except ValueError:\n",
        "            pass\n",
        "\n",
        "    def __iter__(self):\n",
        "        self.cap = cv2.VideoCapture(self.file_name)\n",
        "        if not self.cap.isOpened():\n",
        "            raise IOError('Video {} cannot be opened'.format(self.file_name))\n",
        "        return self\n",
        "\n",
        "    def __next__(self):\n",
        "        was_read, img = self.cap.read()\n",
        "        if not was_read:\n",
        "            raise StopIteration\n",
        "        return img\n",
        "\n",
        "\n",
        "def infer_fast(net, img, net_input_height_size, stride, upsample_ratio, cpu,\n",
        "               pad_value=(0, 0, 0), img_mean=(128, 128, 128), img_scale=1/256):\n",
        "    height, width, _ = img.shape\n",
        "    scale = net_input_height_size / height\n",
        "\n",
        "    scaled_img = cv2.resize(img, (0, 0), fx=scale, fy=scale, interpolation=cv2.INTER_CUBIC)\n",
        "    scaled_img = normalize(scaled_img, img_mean, img_scale)\n",
        "    min_dims = [net_input_height_size, max(scaled_img.shape[1], net_input_height_size)]\n",
        "    padded_img, pad = pad_width(scaled_img, stride, pad_value, min_dims)\n",
        "\n",
        "    tensor_img = torch.from_numpy(padded_img).permute(2, 0, 1).unsqueeze(0).float()\n",
        "    if not cpu:\n",
        "        tensor_img = tensor_img.cuda()\n",
        "\n",
        "    stages_output = net(tensor_img)\n",
        "\n",
        "    stage2_heatmaps = stages_output[-2]\n",
        "    heatmaps = np.transpose(stage2_heatmaps.squeeze().cpu().data.numpy(), (1, 2, 0))\n",
        "    heatmaps = cv2.resize(heatmaps, (0, 0), fx=upsample_ratio, fy=upsample_ratio, interpolation=cv2.INTER_CUBIC)\n",
        "\n",
        "    stage2_pafs = stages_output[-1]\n",
        "    pafs = np.transpose(stage2_pafs.squeeze().cpu().data.numpy(), (1, 2, 0))\n",
        "    pafs = cv2.resize(pafs, (0, 0), fx=upsample_ratio, fy=upsample_ratio, interpolation=cv2.INTER_CUBIC)\n",
        "\n",
        "    return heatmaps, pafs, scale, pad\n",
        "\n",
        "\n",
        "def run_demo(net, image_provider, height_size, cpu, track, smooth):\n",
        "    net = net.eval()\n",
        "    if not cpu:\n",
        "        net = net.cuda()\n",
        "\n",
        "    stride = 8\n",
        "    upsample_ratio = 4\n",
        "    num_keypoints = Pose.num_kpts\n",
        "    previous_poses = []\n",
        "    delay = 33\n",
        "\n",
        "    frame = image_provider.__iter__().__next__()\n",
        "    vid_writer = cv2.VideoWriter('output.avi',cv2.VideoWriter_fourcc('M','J','P','G'), 10, (frame.shape[1],frame.shape[0]))\n",
        "\n",
        "    for img in image_provider:\n",
        "        orig_img = img.copy()\n",
        "        heatmaps, pafs, scale, pad = infer_fast(net, img, height_size, stride, upsample_ratio, cpu)\n",
        "\n",
        "        total_keypoints_num = 0\n",
        "        all_keypoints_by_type = []\n",
        "        for kpt_idx in range(num_keypoints):  # 19th for bg\n",
        "            total_keypoints_num += extract_keypoints(heatmaps[:, :, kpt_idx], all_keypoints_by_type, total_keypoints_num)\n",
        "\n",
        "        pose_entries, all_keypoints = group_keypoints(all_keypoints_by_type, pafs, demo=True)\n",
        "        for kpt_id in range(all_keypoints.shape[0]):\n",
        "            all_keypoints[kpt_id, 0] = (all_keypoints[kpt_id, 0] * stride / upsample_ratio - pad[1]) / scale\n",
        "            all_keypoints[kpt_id, 1] = (all_keypoints[kpt_id, 1] * stride / upsample_ratio - pad[0]) / scale\n",
        "        current_poses = []\n",
        "        for n in range(len(pose_entries)):\n",
        "            if len(pose_entries[n]) == 0:\n",
        "                continue\n",
        "            pose_keypoints = np.ones((num_keypoints, 2), dtype=np.int32) * -1\n",
        "            for kpt_id in range(num_keypoints):\n",
        "                if pose_entries[n][kpt_id] != -1.0:  # keypoint was found\n",
        "                    pose_keypoints[kpt_id, 0] = int(all_keypoints[int(pose_entries[n][kpt_id]), 0])\n",
        "                    pose_keypoints[kpt_id, 1] = int(all_keypoints[int(pose_entries[n][kpt_id]), 1])\n",
        "            pose = Pose(pose_keypoints, pose_entries[n][18])\n",
        "            current_poses.append(pose)\n",
        "\n",
        "        if track:\n",
        "            track_poses(previous_poses, current_poses, smooth=smooth)\n",
        "            previous_poses = current_poses\n",
        "        for pose in current_poses:\n",
        "            pose.draw(img)\n",
        "        img = cv2.addWeighted(orig_img, 0.6, img, 0.4, 0)\n",
        "        for pose in current_poses:\n",
        "            cv2.rectangle(img, (pose.bbox[0], pose.bbox[1]),\n",
        "                          (pose.bbox[0] + pose.bbox[2], pose.bbox[1] + pose.bbox[3]), (0, 255, 0))\n",
        "            if track:\n",
        "                cv2.putText(img, 'id: {}'.format(pose.id), (pose.bbox[0], pose.bbox[1] - 16),\n",
        "                            cv2.FONT_HERSHEY_COMPLEX, 0.5, (0, 0, 255))\n",
        "        #cv2.imshow('Lightweight Human Pose Estimation Python Demo', img)\n",
        "        vid_writer.write(img)\n",
        "        key = cv2.waitKey(delay)\n",
        "        if key == 27:  # esc\n",
        "            return\n",
        "        elif key == 112:  # 'p'\n",
        "            if delay == 33:\n",
        "                delay = 0\n",
        "            else:\n",
        "                delay = 33\n",
        "    vid_writer.release()\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    parser = argparse.ArgumentParser(\n",
        "        description='''Lightweight human pose estimation python demo.\n",
        "                       This is just for quick results preview.\n",
        "                       Please, consider c++ demo for the best performance.''')\n",
        "    parser.add_argument('--checkpoint-path', type=str, required=True, help='path to the checkpoint')\n",
        "    parser.add_argument('--height-size', type=int, default=256, help='network input layer height size')\n",
        "    parser.add_argument('--video', type=str, default='', help='path to video file or camera id')\n",
        "    parser.add_argument('--images', nargs='+', default='', help='path to input image(s)')\n",
        "    parser.add_argument('--cpu', action='store_true', help='run network inference on cpu')\n",
        "    parser.add_argument('--track', type=int, default=1, help='track pose id in video')\n",
        "    parser.add_argument('--smooth', type=int, default=1, help='smooth pose keypoints')\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    if args.video == '' and args.images == '':\n",
        "        raise ValueError('Either --video or --image has to be provided')\n",
        "\n",
        "    net = PoseEstimationWithMobileNet()\n",
        "    checkpoint = torch.load(args.checkpoint_path, map_location='cpu')\n",
        "    load_state(net, checkpoint)\n",
        "\n",
        "    frame_provider = ImageReader(args.images)\n",
        "    if args.video != '':\n",
        "        frame_provider = VideoReader(args.video)\n",
        "    else:\n",
        "        args.track = 0\n",
        "\n",
        "    run_demo(net, frame_provider, args.height_size, args.cpu, args.track, args.smooth)\n"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing demo_test.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b41vnRSNV_ql"
      },
      "source": [
        "## run demo_test.py\n",
        " - upload video\n",
        " - run"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0U6_H9mK_jrZ",
        "outputId": "6e0bea48-ef24-41f6-8cd5-ba3131f0671f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "%%shell\n",
        "rm -rf videos\n",
        "mkdir videos; cd videos\n",
        "curl -LJO https://github.com/jeeenn/SportsFriends/raw/master/Data/yogaFriend_demo.mp4  > test.mp4\n",
        "cd videos\n",
        "mkdir save"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
            "100 2821k  100 2821k    0     0  2396k      0  0:00:01  0:00:01 --:--:-- 62.6M\n",
            "/bin/bash: line 4: cd: videos: No such file or directory\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_kbIDOKc9dtN",
        "outputId": "4ca07bf8-4f51-4773-a5c0-d12c4291dc51",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# GPU\n",
        "!python demo_test.py --checkpoint-path $pre_model_path/checkpoint_iter_370000.pth --video '/content/lightweight-human-pose-estimation.pytorch/videos/yogaFriend_demo.mp4'\n",
        "# CPU\n",
        "#!python demo_test.py --checkpoint-path $pre_model_path/checkpoint_iter_370000.pth --cpu --video '/content/lightweight-human-pose-estimation.pytorch/videos/yogaFriend_demo.mp4'"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/lightweight-human-pose-estimation.pytorch/demo_test.py:162: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  checkpoint = torch.load(args.checkpoint_path, map_location='cpu')\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/lightweight-human-pose-estimation.pytorch/demo_test.py\", line 171, in <module>\n",
            "    run_demo(net, frame_provider, args.height_size, args.cpu, args.track, args.smooth)\n",
            "  File \"/content/lightweight-human-pose-estimation.pytorch/demo_test.py\", line 104, in run_demo\n",
            "    pose_entries, all_keypoints = group_keypoints(all_keypoints_by_type, pafs, demo=True)\n",
            "TypeError: group_keypoints() got an unexpected keyword argument 'demo'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cvgN_n_pCAQB",
        "outputId": "f6b8ea58-7fc2-41ac-b186-a4325bb13101",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!ffmpeg -y -loglevel info -i 'output.avi' openpose.mp4"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ffmpeg version 4.4.2-0ubuntu0.22.04.1 Copyright (c) 2000-2021 the FFmpeg developers\n",
            "  built with gcc 11 (Ubuntu 11.2.0-19ubuntu1)\n",
            "  configuration: --prefix=/usr --extra-version=0ubuntu0.22.04.1 --toolchain=hardened --libdir=/usr/lib/x86_64-linux-gnu --incdir=/usr/include/x86_64-linux-gnu --arch=amd64 --enable-gpl --disable-stripping --enable-gnutls --enable-ladspa --enable-libaom --enable-libass --enable-libbluray --enable-libbs2b --enable-libcaca --enable-libcdio --enable-libcodec2 --enable-libdav1d --enable-libflite --enable-libfontconfig --enable-libfreetype --enable-libfribidi --enable-libgme --enable-libgsm --enable-libjack --enable-libmp3lame --enable-libmysofa --enable-libopenjpeg --enable-libopenmpt --enable-libopus --enable-libpulse --enable-librabbitmq --enable-librubberband --enable-libshine --enable-libsnappy --enable-libsoxr --enable-libspeex --enable-libsrt --enable-libssh --enable-libtheora --enable-libtwolame --enable-libvidstab --enable-libvorbis --enable-libvpx --enable-libwebp --enable-libx265 --enable-libxml2 --enable-libxvid --enable-libzimg --enable-libzmq --enable-libzvbi --enable-lv2 --enable-omx --enable-openal --enable-opencl --enable-opengl --enable-sdl2 --enable-pocketsphinx --enable-librsvg --enable-libmfx --enable-libdc1394 --enable-libdrm --enable-libiec61883 --enable-chromaprint --enable-frei0r --enable-libx264 --enable-shared\n",
            "  libavutil      56. 70.100 / 56. 70.100\n",
            "  libavcodec     58.134.100 / 58.134.100\n",
            "  libavformat    58. 76.100 / 58. 76.100\n",
            "  libavdevice    58. 13.100 / 58. 13.100\n",
            "  libavfilter     7.110.100 /  7.110.100\n",
            "  libswscale      5.  9.100 /  5.  9.100\n",
            "  libswresample   3.  9.100 /  3.  9.100\n",
            "  libpostproc    55.  9.100 / 55.  9.100\n",
            "\u001b[0;35m[avi @ 0x58277d76f000] \u001b[0m\u001b[0;33mCould not find codec parameters for stream 0 (Video: mjpeg (MJPG / 0x47504A4D), none(bt470bg/unknown/unknown), 1920x1080): unspecified pixel format\n",
            "Consider increasing the value for the 'analyzeduration' (0) and 'probesize' (5000000) options\n",
            "\u001b[0mInput #0, avi, from 'output.avi':\n",
            "  Metadata:\n",
            "    software        : Lavf59.27.100\n",
            "  Duration: N/A, start: 0.000000, bitrate: N/A\n",
            "  Stream #0:0: Video: mjpeg (MJPG / 0x47504A4D), none(bt470bg/unknown/unknown), 1920x1080, 10 fps, 10 tbr, 10 tbn, 10 tbc\n",
            "Stream mapping:\n",
            "  Stream #0:0 -> #0:0 (mjpeg (native) -> h264 (libx264))\n",
            "Press [q] to stop, [?] for help\n",
            "\u001b[1;31mCannot determine format of input stream 0:0 after EOF\n",
            "\u001b[0m\u001b[4;31mError marking filters as finished\n",
            "\u001b[0mConversion failed!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TNyFOGN1CG9f",
        "outputId": "17d88308-7a86-41aa-ed89-547085155487",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 502
        }
      },
      "source": [
        "import io\n",
        "import base64\n",
        "from IPython.display import HTML\n",
        "file_name = \"openpose.mp4\"\n",
        "video_encoded = base64.b64encode(io.open(file_name, 'rb').read())\n",
        "HTML(data='''<video width=\"{0}\" height=\"{1}\" alt=\"test\" controls>\n",
        "                      <source src=\"data:video/mp4;base64,{2}\" type=\"video/mp4\" />\n",
        "                    </video>'''.format(640, 480, video_encoded.decode('ascii')))"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<video width=\"640\" height=\"480\" alt=\"test\" controls>\n",
              "                      <source src=\"data:video/mp4;base64,\" type=\"video/mp4\" />\n",
              "                    </video>"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xO2_mgaZ8opC"
      },
      "source": [
        "# Real Time Pose Detection\n",
        "  - colab ( ing )"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xkoANTZRKzCZ"
      },
      "source": [
        "import base64, logging\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from io import BytesIO\n",
        "\n",
        "def data_uri_to_img(uri):\n",
        "  try:\n",
        "    image = base64.b64decode(uri.split(',')[1], validate=True)\n",
        "    # make the binary image, a PIL image\n",
        "    image = Image.open(BytesIO(image))\n",
        "    # convert to numpy array\n",
        "    image = np.array(image, dtype=np.uint8);\n",
        "    return image\n",
        "  except Exception as e:\n",
        "    logging.exception(e);print('\\n')\n",
        "    return None\n",
        "\n",
        "def video_to_data_url(filename):\n",
        "    ext = filename.split('.')[-1]\n",
        "    prefix = 'data:video/{};base64,'.format(ext)\n",
        "    with open(filename, 'rb') as f:\n",
        "        vidoe = f.read()\n",
        "    return prefix + base64.b64encode(vidoe).decode()\n"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gXLF8WsEcxKY"
      },
      "source": [
        "import cv2 as cv\n",
        "import torch\n",
        "\n",
        "from models.with_mobilenet import PoseEstimationWithMobileNet\n",
        "from modules.keypoints import extract_keypoints, group_keypoints\n",
        "from modules.load_state import load_state\n",
        "from modules.pose import Pose, track_poses\n",
        "from val import normalize, pad_width\n",
        "\n",
        "def infer_fast(net, img, net_input_height_size, stride, upsample_ratio, cpu,\n",
        "               pad_value=(0, 0, 0), img_mean=(128, 128, 128), img_scale=1/256):\n",
        "    height, width, _ = img.shape\n",
        "    scale = net_input_height_size / height\n",
        "\n",
        "    scaled_img = cv.resize(img, (0, 0), fx=scale, fy=scale, interpolation=cv.INTER_CUBIC)\n",
        "    scaled_img = normalize(scaled_img, img_mean, img_scale)\n",
        "    min_dims = [net_input_height_size, max(scaled_img.shape[1], net_input_height_size)]\n",
        "    padded_img, pad = pad_width(scaled_img, stride, pad_value, min_dims)\n",
        "\n",
        "    tensor_img = torch.from_numpy(padded_img).permute(2, 0, 1).unsqueeze(0).float()\n",
        "    if not cpu:\n",
        "        tensor_img = tensor_img.cuda()\n",
        "\n",
        "    stages_output = net(tensor_img)\n",
        "\n",
        "    stage2_heatmaps = stages_output[-2]\n",
        "    heatmaps = np.transpose(stage2_heatmaps.squeeze().cpu().data.numpy(), (1, 2, 0))\n",
        "    heatmaps = cv2.resize(heatmaps, (0, 0), fx=upsample_ratio, fy=upsample_ratio, interpolation=cv2.INTER_CUBIC)\n",
        "\n",
        "    stage2_pafs = stages_output[-1]\n",
        "    pafs = np.transpose(stage2_pafs.squeeze().cpu().data.numpy(), (1, 2, 0))\n",
        "    pafs = cv2.resize(pafs, (0, 0), fx=upsample_ratio, fy=upsample_ratio, interpolation=cv2.INTER_CUBIC)\n",
        "\n",
        "    return heatmaps, pafs, scale, pad\n"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DGZsZuH2K7n8"
      },
      "source": [
        "def detect_key_point(frame):\n",
        "  #(frameWidth, frameHeight) = frame.shape[:2]\n",
        "  #inp = cv.dnn.blobFromImage(frame, inScale, (inWidth, inHeight),\n",
        "  #                            (0, 0, 0), swapRB=False, crop=False)\n",
        "\n",
        "  model_path = os.path.join(pre_model_path,'checkpoint_iter_370000.pth')\n",
        "  height_size = 1\n",
        "  cpu = True\n",
        "\n",
        "  net = PoseEstimationWithMobileNet()\n",
        "  checkpoint = torch.load(model_path, map_location='cpu')\n",
        "  load_state(net, checkpoint)\n",
        "  #net = cv.dnn.readNetFromModelOptimizer(protoFile, weightsFile)\n",
        "  net = net.eval()\n",
        "  if not cpu:\n",
        "      net = net.cuda()\n",
        "\n",
        "  stride = 8\n",
        "  upsample_ratio = 4\n",
        "  num_keypoints = Pose.num_kpts\n",
        "  previous_poses = []\n",
        "  delay = 33\n",
        "  orig_img = img.copy()\n",
        "  heatmaps, pafs, scale, pad = infer_fast(net, frame, height_size, stride, upsample_ratio, cpu)\n",
        "\n",
        "  total_keypoints_num = 0\n",
        "  all_keypoints_by_type = []\n",
        "  for kpt_idx in range(num_keypoints):  # 19th for bg\n",
        "      total_keypoints_num += extract_keypoints(heatmaps[:, :, kpt_idx], all_keypoints_by_type, total_keypoints_num)\n",
        "\n",
        "  pose_entries, all_keypoints = group_keypoints(all_keypoints_by_type, pafs, demo=True)\n",
        "  for kpt_id in range(all_keypoints.shape[0]):\n",
        "      all_keypoints[kpt_id, 0] = (all_keypoints[kpt_id, 0] * stride / upsample_ratio - pad[1]) / scale\n",
        "      all_keypoints[kpt_id, 1] = (all_keypoints[kpt_id, 1] * stride / upsample_ratio - pad[0]) / scale\n",
        "  current_poses = []\n",
        "  for n in range(len(pose_entries)):\n",
        "      if len(pose_entries[n]) == 0:\n",
        "          continue\n",
        "      pose_keypoints = np.ones((num_keypoints, 2), dtype=np.int32) * -1\n",
        "      for kpt_id in range(num_keypoints):\n",
        "          if pose_entries[n][kpt_id] != -1.0:  # keypoint was found\n",
        "              pose_keypoints[kpt_id, 0] = int(all_keypoints[int(pose_entries[n][kpt_id]), 0])\n",
        "              pose_keypoints[kpt_id, 1] = int(all_keypoints[int(pose_entries[n][kpt_id]), 1])\n",
        "      pose = Pose(pose_keypoints, pose_entries[n][18])\n",
        "      current_poses.append(pose)\n",
        "\n",
        "      if track:\n",
        "          track_poses(previous_poses, current_poses, smooth=smooth)\n",
        "          previous_poses = current_poses\n",
        "      for pose in current_poses:\n",
        "          pose.draw(img)\n",
        "      img = cv.addWeighted(orig_img, 0.6, img, 0.4, 0)\n",
        "      for pose in current_poses:\n",
        "          cv.rectangle(img, (pose.bbox[0], pose.bbox[1]),\n",
        "                        (pose.bbox[0] + pose.bbox[2], pose.bbox[1] + pose.bbox[3]), (0, 255, 0))\n",
        "          if track:\n",
        "              cv.putText(img, 'id: {}'.format(pose.id), (pose.bbox[0], pose.bbox[1] - 16),\n",
        "                          cv.FONT_HERSHEY_COMPLEX, 0.5, (0, 0, 255))\n"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lHcPLEPfFRpR"
      },
      "source": [
        "from IPython.display import display, Javascript\n",
        "from google.colab.output import eval_js\n",
        "from base64 import b64decode\n",
        "\n",
        "def take_photo(filename='photo.jpg', quality=0.8):\n",
        "  js = Javascript('''\n",
        "    async function takePhoto(quality) {\n",
        "      const div = document.createElement('div');\n",
        "      const capture = document.createElement('button');\n",
        "      capture.textContent = 'Capture';\n",
        "      div.appendChild(capture);\n",
        "\n",
        "      const video = document.createElement('video');\n",
        "      video.style.display = 'block';\n",
        "      const stream = await navigator.mediaDevices.getUserMedia({video: true});\n",
        "\n",
        "      document.body.appendChild(div);\n",
        "      div.appendChild(video);\n",
        "      video.srcObject = stream;\n",
        "      await video.play();\n",
        "\n",
        "      // Resize the output to fit the video element.\n",
        "      google.colab.output.setIframeHeight(document.documentElement.scrollHeight, true);\n",
        "\n",
        "      // Wait for Capture to be clicked.\n",
        "      await new Promise((resolve) => capture.onclick = resolve);\n",
        "\n",
        "      const canvas = document.createElement('canvas');\n",
        "      canvas.width = video.videoWidth;\n",
        "      canvas.height = video.videoHeight;\n",
        "      canvas.getContext('2d').drawImage(video, 0, 0);\n",
        "      stream.getVideoTracks()[0].stop();\n",
        "\n",
        "      for (let i = 0; isOpened; i++) {\n",
        "        canvas.getContext(\"2d\").clearRect(0, 0, canvas.width, canvas.height);\n",
        "        canvas.getContext('2d').drawImage(video, 0, 0);\n",
        "        img = canvas.toDataURL('image/jpeg', quality);\n",
        "\n",
        "        // jsLog(i + \"sending\");\n",
        "        // Call a python function and send this image\n",
        "        google.colab.kernel.invokeFunction('notebook.run_PoseDetection', [img], {});\n",
        "        // jsLog(i + \"SENT\");\n",
        "\n",
        "        // wait for X miliseconds second, before next capture\n",
        "        await new Promise(resolve => setTimeout(resolve, 250));\n",
        "      }\n",
        "\n",
        "      div.remove();\n",
        "      return canvas.toDataURL('image/jpeg', quality);\n",
        "    }\n",
        "    ''')\n",
        "  display(js)\n",
        "  data = eval_js('takePhoto({})'.format(quality))\n",
        "  binary = b64decode(data.split(',')[1])\n",
        "  with open(filename, 'wb') as f:\n",
        "    f.write(binary)\n",
        "  return filename"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qrnbfvXMIOhK"
      },
      "source": [
        "from google.colab import output\n",
        "frame_count = 0\n",
        "writer = None\n",
        "\n",
        "# InvokeFunction\n",
        "# takes the numpy image and runs detection, then shows the results by visualizing\n",
        "def run_PoseDetection(uri):\n",
        "  global frame_count, writer\n",
        "\n",
        "  image = data_uri_to_img(uri)\n",
        "  if writer is None:\n",
        "      fourcc = cv.VideoWriter_fourcc(*'DIVX')\n",
        "      writer = cv.VideoWriter(outVideo, fourcc, 2, (image.shape[1], image.shape[0]), True)\n",
        "  try:\n",
        "    detect_key_point(image)\n",
        "\n",
        "    image = cv.cvtColor(image, cv.COLOR_BGR2RGB)\n",
        "    frame_count+=1\n",
        "    name = '{0}.jpg'.format(frame_count)\n",
        "    name = os.path.join(VIDEO_SAVE_DIR, name)\n",
        "    cv.imwrite(name, image)\n",
        "\n",
        "    if writer is not None:\n",
        "      writer.write(image)\n",
        "  except Exception as e:\n",
        "    logging.exception(e)\n",
        "    print('\\n')\n",
        "\n",
        "# register this function, so JS code could call this\n",
        "output.register_callback('notebook.run_PoseDetection', run_PoseDetection)"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "92rtB6kTZM-5"
      },
      "source": [
        "import os\n",
        "\n",
        "VIDEO_DIR = os.path.join(git_path, \"videos\")\n",
        "VIDEO_SAVE_DIR = os.path.join(VIDEO_DIR, \"save\")"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EJqu9t_jMqEJ"
      },
      "source": [
        "inVideo = os.path.join(VIDEO_DIR, \"test.mp4\")\n",
        "outVideo= os.path.join(VIDEO_DIR, \"out.avi\")"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ATny_8DGaCpT",
        "outputId": "e5c13e4c-01e5-4441-b647-00f83915f1f2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 495
        }
      },
      "source": [
        "%%shell\n",
        "mkdir videos; cd videos\n",
        "curl -LJO https://github.com/jeeenn/SportsFriends/raw/master/Data/yogaFriend_demo.mp4  > test.mp4\n",
        "cd videos\n",
        "mkdir save"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‘videos’: File exists\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
            "\r100 2821k  100 2821k    0     0  15.7M      0 --:--:-- --:--:-- --:--:-- 15.7M\n",
            "/bin/bash: line 3: cd: videos: No such file or directory\n",
            "mkdir: cannot create directory ‘save’: File exists\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "CalledProcessError",
          "evalue": "Command 'mkdir videos; cd videos\ncurl -LJO https://github.com/jeeenn/SportsFriends/raw/master/Data/yogaFriend_demo.mp4  > test.mp4\ncd videos\nmkdir save\n' returned non-zero exit status 1.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mCalledProcessError\u001b[0m                        Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-37-4623ed25f4cc>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'shell'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'mkdir videos; cd videos\\ncurl -LJO https://github.com/jeeenn/SportsFriends/raw/master/Data/yogaFriend_demo.mp4  > test.mp4\\ncd videos\\nmkdir save\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_shell.py\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m    332\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m       \u001b[0mcell\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 334\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmagic_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    335\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2471\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2472\u001b[0m                 \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2473\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2474\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2475\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_system_commands.py\u001b[0m in \u001b[0;36m_shell_cell_magic\u001b[0;34m(args, cmd)\u001b[0m\n\u001b[1;32m    110\u001b[0m   \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_run_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcmd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclear_streamed_output\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mparsed_args\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mignore_errors\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m     \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_returncode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_system_commands.py\u001b[0m in \u001b[0;36mcheck_returncode\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    135\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcheck_returncode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m       raise subprocess.CalledProcessError(\n\u001b[0m\u001b[1;32m    138\u001b[0m           \u001b[0mreturncode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m       )\n",
            "\u001b[0;31mCalledProcessError\u001b[0m: Command 'mkdir videos; cd videos\ncurl -LJO https://github.com/jeeenn/SportsFriends/raw/master/Data/yogaFriend_demo.mp4  > test.mp4\ncd videos\nmkdir save\n' returned non-zero exit status 1."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MSXZy5SkLKxE",
        "outputId": "4d7a86f9-ef93-402b-801c-e0922498b136",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 744
        }
      },
      "source": [
        "frame_count = 0\n",
        "data_url = video_to_data_url(inVideo)\n",
        "try:\n",
        "  # put the JS code in cell and run it\n",
        "  take_photo(data_url)\n",
        "  if writer is not None:\n",
        "    writer.release()\n",
        "except Exception as e:\n",
        "  logging.exception(e)\n",
        "  print('\\n')\n",
        "\n",
        "writer = None"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function takePhoto(quality) {\n",
              "      const div = document.createElement('div');\n",
              "      const capture = document.createElement('button');\n",
              "      capture.textContent = 'Capture';\n",
              "      div.appendChild(capture);\n",
              "\n",
              "      const video = document.createElement('video');\n",
              "      video.style.display = 'block';\n",
              "      const stream = await navigator.mediaDevices.getUserMedia({video: true});\n",
              "\n",
              "      document.body.appendChild(div);\n",
              "      div.appendChild(video);\n",
              "      video.srcObject = stream;\n",
              "      await video.play();\n",
              "\n",
              "      // Resize the output to fit the video element.\n",
              "      google.colab.output.setIframeHeight(document.documentElement.scrollHeight, true);\n",
              "\n",
              "      // Wait for Capture to be clicked.\n",
              "      await new Promise((resolve) => capture.onclick = resolve);\n",
              "\n",
              "      const canvas = document.createElement('canvas');\n",
              "      canvas.width = video.videoWidth;\n",
              "      canvas.height = video.videoHeight;\n",
              "      canvas.getContext('2d').drawImage(video, 0, 0);\n",
              "      stream.getVideoTracks()[0].stop();\n",
              "\n",
              "      for (let i = 0; isOpened; i++) {\n",
              "        canvas.getContext(\"2d\").clearRect(0, 0, canvas.width, canvas.height);\n",
              "        canvas.getContext('2d').drawImage(video, 0, 0);\n",
              "        img = canvas.toDataURL('image/jpeg', quality);\n",
              "\n",
              "        // jsLog(i + \"sending\");\n",
              "        // Call a python function and send this image\n",
              "        google.colab.kernel.invokeFunction('notebook.run_PoseDetection', [img], {});\n",
              "        // jsLog(i + \"SENT\");\n",
              "\n",
              "        // wait for X miliseconds second, before next capture\n",
              "        await new Promise(resolve => setTimeout(resolve, 250));        \n",
              "      }       \n",
              "\n",
              "      div.remove();\n",
              "      return canvas.toDataURL('image/jpeg', quality);\n",
              "    }\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:root:ReferenceError: isOpened is not defined\n",
            "Traceback (most recent call last):\n",
            "  File \"<ipython-input-38-8521a4e8e319>\", line 5, in <cell line: 3>\n",
            "    take_photo(data_url)\n",
            "  File \"<ipython-input-33-0bf0092f58e0>\", line 53, in take_photo\n",
            "    data = eval_js('takePhoto({})'.format(quality))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/google/colab/output/_js.py\", line 40, in eval_js\n",
            "    return _message.read_reply_from_input(request_id, timeout_sec)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/google/colab/_message.py\", line 103, in read_reply_from_input\n",
            "    raise MessageError(reply['error'])\n",
            "google.colab._message.MessageError: ReferenceError: isOpened is not defined\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2 as cv"
      ],
      "metadata": {
        "id": "8Ba_S5z3pSsz"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from models.with_mobilenet import PoseEstimationWithMobileNet\n",
        "from datasets.transformations import Scale, Rotate, Flip, CropPad, ConvertKeypoints\n",
        "\n",
        "\n",
        "import os\n",
        "\n",
        "model_path = \"/content/lightweight-human-pose-estimation.pytorch/models/with_mobilenet.py\"\n",
        "\n",
        "def load_model(model_path):\n",
        "    load_model(\"/content/lightweight-human-pose-estimation.pytorch/models/with_mobilenet.py\")\n",
        "    # Load the pre-trained model\n",
        "    model = pose_hrnet.get_pose_net()\n",
        "    checkpoint = torch.load('/content/lightweight-human-pose-estimation.pytorch/models/with_mobilenet.py')\n",
        "    model.load_state_dict(checkpoint['state_dict'])\n",
        "    model.eval()\n",
        "    return model\n",
        "\n",
        "image_path = \"/content/WhatsApp Image 2024-10-20 at 17.19.49_7b8c66cd (2).jpg\"\n",
        "\n",
        "def preprocess_image(image_path):\n",
        "    # Read the image and convert it to RGB\n",
        "    image = cv2.imread(image_path)\n",
        "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    # Normalize and resize the image\n",
        "    image = cv2.resize(image, (256, 192))  # Default input size for HRNet\n",
        "    image = image.astype(np.float32) / 255.0\n",
        "    return image\n",
        "\n",
        "def postprocess_output(output, image_shape):\n",
        "    # Assuming output contains keypoint locations and heatmaps\n",
        "    preds, _ = get_final_preds(output, center=None, scale=None)\n",
        "\n",
        "    # Scale the predictions back to the original image dimensions\n",
        "    preds = preds * np.array([image_shape[1], image_shape[0]])  # Convert to image space\n",
        "    return preds\n",
        "\n",
        "def plot_pose(image, keypoints):\n",
        "    # Visualize the pose estimation\n",
        "    plt.imshow(image)\n",
        "    for point in keypoints:\n",
        "        x, y = point\n",
        "        plt.scatter(x, y, color='red', s=10)\n",
        "    plt.show()\n",
        "\n",
        "def apply_pose_estimation(image_path, model_path):\n",
        "    # Load the image and preprocess it\n",
        "    image = preprocess_image(image_path)\n",
        "\n",
        "    # Load the model\n",
        "    model = load_model(model_path)\n",
        "\n",
        "    # Convert image to tensor and add batch dimension\n",
        "    input_image = torch.from_numpy(image).permute(2, 0, 1).unsqueeze(0)\n",
        "\n",
        "    # Run inference on the image\n",
        "    with torch.no_grad():\n",
        "        output = model(input_image)\n",
        "\n",
        "    # Postprocess the output\n",
        "    keypoints = postprocess_output(output, image.shape)\n",
        "\n",
        "    # Plot the keypoints on the image\n",
        "    plot_pose(image, keypoints)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    image_path = \"path_to_your_image.jpg\"  # Replace with your image path\n",
        "    model_path = \"path_to_your_pretrained_model.pth\"  # Replace with the path to the pretrained model\n",
        "    apply_pose_estimation(image_path, model_path)"
      ],
      "metadata": {
        "id": "ZbflF8ijhfZT",
        "outputId": "97ecf413-d2f7-42a7-94b8-dab3b9447f19",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        }
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'cv2' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-47-d632304611e5>\u001b[0m in \u001b[0;36m<cell line: 67>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0mimage_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"path_to_your_image.jpg\"\u001b[0m  \u001b[0;31m# Replace with your image path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[0mmodel_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"path_to_your_pretrained_model.pth\"\u001b[0m  \u001b[0;31m# Replace with the path to the pretrained model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m     \u001b[0mapply_pose_estimation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-47-d632304611e5>\u001b[0m in \u001b[0;36mapply_pose_estimation\u001b[0;34m(image_path, model_path)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mapply_pose_estimation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0;31m# Load the image and preprocess it\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m     \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocess_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0;31m# Load the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-47-d632304611e5>\u001b[0m in \u001b[0;36mpreprocess_image\u001b[0;34m(image_path)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mpreprocess_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;31m# Read the image and convert it to RGB\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m     \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcvtColor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCOLOR_BGR2RGB\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'cv2' is not defined"
          ]
        }
      ]
    }
  ]
}